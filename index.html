<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Nim AI</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link rel="stylesheet" href="style.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=DM+Mono:ital,wght@0,400;0,500;1,300&family=Roboto+Mono:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <script src="script.js" type="module" defer></script>
  </head>
  <body>
    <div class="container">
      <div id="info">
        The human goes first. On each turn, you must remove at least one stick, and may remove any number of sticks provided they are all on the same row. The player who removes the last stick loses.
      </br><br>
    After you are done, press 'AI move' for the AI to make a move. The AI is trained by reinforcement learning (Q-learning).
    </br><br>
     By playing against itself for 10,000 games, and assigning rewards and punishments to its moves, the AI is able to learn the optimal move to play at any state. To balance exploration and exploitation, the AI chooses its moves based on the epsilon-greedy algorithm.
    </br><br>
    Toggle computation to allow the AI to train against itself in real time (check the console). Else by default, the AI takes the optimal move from a pre-trained Q-value table so as to save your computing resources.
  </br><br>
    Source code: <a href="https://github.com/Elliott-Chong/Nim-Q-Learning-AI">Elliott Chong</a>
      </div>
      <div id="training">
      </div>
    </div>
    <div id="game-over-screen" class="hidden">
      <h1>Game Over!</h1>
      <button class="btn" id="restart-btn">Restart</button>
    </div>
  </body>
</html>
